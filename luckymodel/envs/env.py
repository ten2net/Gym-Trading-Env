import sys
sys.path.append("../")
from envs.feature_engineering import FeatureEngineer
import warnings
from gym_trading_env.environments import TradingEnv
from stable_baselines3.common.monitor import Monitor
import gymnasium as gym
import numpy as np
import pandas as pd
from typing import Optional, List, Literal
import argparse



warnings.filterwarnings("ignore", category=ResourceWarning)

def calculate_reward(
    current_value: float,
    prev_value: float,
    step: int,
    max_steps: int = 480,
    target_profit: float = 0.15,
    stop_loss: float = -0.1,
    consecutive_ups: int = 0,
    consecutive_downs: int = 0
) -> tuple[float, bool, bool]:
    """
    å¼ºåŒ–å­¦ä¹ äº¤æ˜“ç­–ç•¥çš„å¥–åŠ±è®¡ç®—å‡½æ•°
    
    å‚æ•°ï¼š
    - current_value: å½“å‰èµ„äº§å‡€å€¼ï¼ˆæ ‡å‡†åŒ–åçš„å€¼ï¼Œåˆå§‹ä¸º1.0ï¼‰
    - prev_value: å‰ä¸€æ­¥çš„èµ„äº§å‡€å€¼
    - step: å½“å‰æ­¥æ•°ï¼ˆ0-basedï¼‰
    - max_steps: æœ€å¤§å…è®¸æ­¥æ•°
    
    è¿”å›ï¼š
    - reward: å½“å‰æ­¥çš„å¥–åŠ±å€¼
    - done: æ˜¯å¦ç»ˆæ­¢å½“å‰å›åˆ
    """
    # ========== å‚æ•°é…ç½® ==========
    TARGET_PROFIT = target_profit   # ç›®æ ‡æ”¶ç›Šç‡15%
    STOP_LOSS = stop_loss      # æœ€å¤§äºæŸ10%
    
    # è®¡ç®—å½“å‰æ”¶ç›ŠçŠ¶æ€
    current_return = current_value - 1.0   # æ ‡å‡†åŒ–æ”¶ç›Šç‡
    prev_return = prev_value - 1.0  if prev_value is not None else 0.0
    momentum = current_return - prev_return
    
    # å¥–åŠ±ç³»æ•°é…ç½®    
    PROFIT_STEP_COEFF = 2.0
    LOSS_STEP_COEFF = 2 * PROFIT_STEP_COEFF    
    TARGET_BONUS_BASE = 60 * PROFIT_STEP_COEFF
    STOP_LOSS_PENALTY_BASE = -10 * LOSS_STEP_COEFF
    # TARGET_BONUS = TARGET_BONUS_BASE * (1 - (step/max_steps)**0.4)
    # STOP_LOSS_PENALTY = STOP_LOSS_PENALTY_BASE * (step/max_steps)**1.2

    done, truncated = False, False
    reward = 0.0
    rewards = []
    # ----------------------------
    # åŸºç¡€å¥–åŠ±è®¡ç®—ï¼ˆæ¯æ­¥åŠ¨æ€å¥–åŠ±ï¼‰
    # ----------------------------
    if current_return >= 0:
        # è®¡ç®—æ”¶ç›Šè¿›åº¦å˜åŒ–
        curr_progress = current_return / TARGET_PROFIT
        prev_progress = prev_return / TARGET_PROFIT if prev_return >= 0 else 0.0
        reward += (curr_progress - prev_progress) * PROFIT_STEP_COEFF
    else:
        # è®¡ç®—äºæŸè¿›åº¦å˜åŒ–ï¼ˆç›¸å¯¹æ­¢æŸï¼‰
        curr_loss = current_return/STOP_LOSS
        prev_loss = prev_return/STOP_LOSS if prev_return < 0 else 0.0
        reward -= (curr_loss - prev_loss) * LOSS_STEP_COEFF  # äºæŸæ‰©å¤§åˆ™æƒ©ç½š
    rewards.append(reward)
    
    # æƒ…å†µ1ï¼šè¾¾åˆ°ç›®æ ‡æ”¶ç›Š
    if current_return >= TARGET_PROFIT :
        # print(f"è¾¾åˆ°ç›®æ ‡ {step} {current_return:.4f} {prev_return: .4f} {TARGET_PROFIT: .2f}  {STOP_LOSS: .2f}")
        time_decay = 0.5 + 0.5*(max_steps - step)/max_steps  # è¶Šæ—©å®Œæˆæ•ˆç‡è¶Šé«˜
        reward += TARGET_BONUS_BASE * time_decay 
        done = True
        rewards.append(reward)
    # æƒ…å†µ2ï¼šè§¦å‘æ­¢æŸ
    elif current_return <= STOP_LOSS :
        # print(f"è§¦å‘æ­¢æŸ {step} {current_return:.4f} {prev_return: .4f} {TARGET_PROFIT: .2f}  {STOP_LOSS: .2f}")
        time_decay = 0.3 + 0.7*(max_steps - step)/max_steps # è¶Šæ—©è§¦å‘æƒ©ç½šè¶Šé‡
        reward += STOP_LOSS_PENALTY_BASE  * time_decay
        done = True
        rewards.append(reward)
    # æƒ…å†µ3ï¼šè¾¾åˆ°æœ€å¤§æ­¥æ•°
    elif step >= max_steps :  # è€ƒè™‘0-basedç´¢å¼•
        # print(f"{step} {reward:.4f} {current_return:.4f} {prev_return: .4f} ")
        if current_return >= 0:
            reward += TARGET_BONUS_BASE  * (current_return / TARGET_PROFIT)
        else:
            reward += STOP_LOSS_PENALTY_BASE  * (current_return / STOP_LOSS)
            
        # if current_return < TARGET_PROFIT:  # ğŸŒŸ æœªè¾¾æ ‡è¿½åŠ æƒ©ç½š
        #     reward -= 1 * (1 - current_return/TARGET_PROFIT)  # ç¦»ç›®æ ‡è¶Šè¿œæƒ©ç½šè¶Šå¤§            
        truncated = True
        rewards.append(reward)    
    # æ·»åŠ å®Œæˆé€Ÿåº¦å¥–åŠ±
    if done and current_return >= TARGET_PROFIT and not truncated:
        speed_bonus = 200 * max(1 - (step / (max_steps * 0.8))**0.8, 0) # å‰80%æ­¥æ•°å®Œæˆæœ‰é¢å¤–å¥–åŠ±
        reward += max(speed_bonus, 1)  
        # ä½¿ç”¨æŒ‡æ•°è¡°å‡ä»£æ›¿å¹‚è¡°å‡ï¼Œå‰30%æ­¥æ•°å¥–åŠ±æ›´é«˜
        # speed_bonus = 10 * np.exp(-3.0 * step/(max_steps*0.7))  
        # reward += max(speed_bonus, 10)  # è®¾ç½®æœ€ä½å¥–åŠ±ä¿éšœ              
    rewards.append(reward)
    # ----------------------------
    # è¶‹åŠ¿å¥–åŠ±ï¼ˆæŠ‘åˆ¶éœ‡è¡ï¼‰è¶‹åŠ¿å»¶ç»­å¥–åŠ±å¯ä½¿åæœŸè®­ç»ƒæ›´ç¨³å®š
    # åœ¨ä¸Šå‡è¶‹åŠ¿ä¸­â€‹â€‹ä¿æŒæŒä»“â€‹â€‹
    # åœ¨ä¸‹è·Œè¶‹åŠ¿ä¸­â€‹â€‹åŠ é€Ÿæ­¢æŸâ€‹â€‹
    # åœ¨éœ‡è¡è¡Œæƒ…ä¸­â€‹â€‹å‡å°‘æ— æ•ˆäº¤æ˜“
    # ----------------------------
    # æ›´æ–°è¿ç»­ä¸Šæ¶¨/ä¸‹è·Œæ¬¡æ•°
    new_ups, new_downs = consecutive_ups, consecutive_downs
    if momentum > 0:
        new_ups += 1
        new_downs = 0
    elif momentum < 0:
        new_downs += 1
        new_ups = 0
    else:
        new_ups, new_downs = 0, 0
        
    # æ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´ç³»æ•°ï¼ˆåæœŸé™ä½åŠ¨é‡æ•æ„Ÿæ€§ï¼‰ğŸŒŸ
    progress_ratio = step / max_steps
    momentum_coeff = 0.3 + 0.5*(1 - progress_ratio)  # åˆå§‹0.8ï¼ŒåæœŸé™è‡³0.3
    if abs(momentum) < 0.005:
        momentum_coeff *= 0.5  # å°å¹…æ³¢åŠ¨é¢å¤–é™ä½æ•æ„Ÿæ€§    
    reward += momentum_coeff * momentum  # åŠ¨é‡ç›´æ¥å½±å“å¥–åŠ±
    # è¿ç»­è¶‹åŠ¿å¥–åŠ±
    if new_ups > 0:
        reward += 0.25 * (new_ups ** 0.5)
        if new_ups >= 3:
            reward += 0.8
    elif new_downs > 0:
        reward -= 0.25 * (new_downs ** 0.5)
        if new_downs >= 3:
            reward -= 0.8 
    rewards.append(reward)
    # åœ¨æ‰€æœ‰å¥–åŠ±è®¡ç®—å®Œæˆåæ·»åŠ æ¯”ä¾‹çº¦æŸï¼ˆä¿æŒæ€»å¥–åŠ±åœ¨åˆç†èŒƒå›´ï¼‰ğŸŒŸ
    # total_abs = abs(reward)
    # # print(reward,  rewards)
    # if total_abs > 0:
    #     for component in rewards:
    #         ratio = abs(component)/total_abs
    #         if ratio > 0.6:  # é™åˆ¶å•é¡¹å æ¯”ä¸è¶…è¿‡60%
    #             reward = reward * 0.6/ratio

    return round(reward, 6), done, truncated, new_ups, new_downs


def dynamic_feature_last_position_taken(history):
    return history['position', -1]


def dynamic_feature_real_position(history):
    return history['real_position', -1]


def make_env(
    symbol: str,
    eval: bool = False,
    window_size: int | None = 24,
    positions: List[float] = [0, 0.5, 1],
    trading_fees: float = 0.01/100,  # 0.01% per stock buy / sell
    portfolio_initial_value: float = 1000000.0,  # in FIAT (here, YMB)
    max_episode_duration: int | Literal["max"] = 48 * 22,  # "max" ,# 500,
    target_return: float = 0.15,  # ç­–ç•¥ç›®æ ‡æ”¶ç›Šç‡ï¼Œè¶…è¿‡è§†ä¸ºæˆåŠŸå®Œæˆï¼Œç»™äºˆé«˜é¢å¥–åŠ±
    stop_loss: float = -0.10,  # æ­¢æŸï¼Œä½äºè§†ä¸ºå¤±è´¥ï¼Œç»™äºˆæƒ©ç½š
    render_mode: Literal["logs", "human"] = "logs",
    verbose: Literal[0, 1, 2] = 1
):
    df = pd.read_csv(
        f"../raw_data/csv/m5/{symbol}.csv", parse_dates=["date"], index_col="date")
    df.sort_index(inplace=True)
    df.dropna(inplace=True)
    df.drop_duplicates(inplace=True)

    # Generating features
    # WARNING : the column names need to contain keyword 'feature' !
    df["feature_close"] = 100 * df["close"].pct_change()
    df["feature_open"] = df["open"] - df["close"] / (df["open"] + df["close"])
    df["feature_high"] = df["high"] - df["close"] / (df["high"] + df["close"])
    df["feature_low"] = df["low"] - df["close"] / (df["low"] + df["close"])
    df['dt'] = df.index.date
    # 2. è·å–æ¯æ—¥å¼€ç›˜ä»·
    daily_open = df.groupby('dt')['open'].transform('first')
    # 3. å°†æ¯æ—¥å¼€ç›˜ä»·åˆå¹¶å›åŸå§‹æ•°æ®æ¡†
    df = df.merge(daily_open.rename('daily_open'),
                  left_on='date',
                  right_index=True)
    df['feature_close_open_yoy'] = df['close'] - \
        df['daily_open'] / (df['close'] + df['daily_open'])
    # df["feature_volume"] = df["volume"] / df["volume"].rolling(12).max()
    points_per_day = 48  # 24å°æ—¶*60åˆ†é’Ÿ/5åˆ†é’Ÿ=288ï¼Œä½†å®é™…äº¤æ˜“æ—¶é—´å¯èƒ½æ›´å°‘
    df['close_prev'] = df['close'].shift(points_per_day)
    df['volume_prev'] = df['volume'].shift(points_per_day)
    df['cum_volume'] = df.groupby('dt')['volume'].cumsum()
    df['cum_volume_prev'] = df["cum_volume"].shift(points_per_day)

    df['feature_close_yoy'] = (
        df['close'] - df['close_prev']) / (df['close'] + df['close_prev'])
    df['feature_volume_sum'] = (
        df['cum_volume'] - df['cum_volume_prev']) / (df['cum_volume'] + df['cum_volume_prev'])
    df['feature_volume'] = (df['volume'] - df['volume_prev']) / \
        (df['volume'] + df['volume_prev'])
    df = df.drop(columns=['dt', 'daily_open',
                 'volume_prev', 'cum_volume', 'cum_volume_prev'])
    # print(df[-50:])
    fe = FeatureEngineer(window_size=3)
    df = fe.compute_features(df)
    numeric_cols = df.columns
    for col in numeric_cols:
        if col.startswith("feature"):
            df[col] = df[col].round(3)
    df.dropna(inplace=True)
    df.to_csv(f"{symbol}.csv", index=True, encoding='utf_8_sig')
    env = gym.make(
        "TradingEnv",
        name=symbol,
        df=df,
        initial_position=0,  # 'random', #Initial position
        reward_function=calculate_reward,
        # dynamic_feature_functions = [dynamic_feature_last_position_taken, dynamic_feature_real_position],
        windows=window_size,
        positions=positions,
        trading_fees=trading_fees,
        portfolio_initial_value=portfolio_initial_value,
        max_episode_duration=max_episode_duration,
        target_return=target_return,
        stop_loss=stop_loss,
        disable_env_checker=True,
        render_mode=render_mode,
        verbose=verbose
    )

    # env.add_metric('è°ƒå‚æ¬¡æ•°', lambda history: np.sum(
    #     np.diff(history['real_position']) != 0))
    # env.add_metric(
    #     'å¥–åŠ± sum', lambda history: f"{np.sum(history["reward"]):.4f}")
    # env.add_metric(
    #     'å¥–åŠ± max', lambda history: f"{np.max(history["reward"]):.4f}")
    # env.add_metric(
    #     'å¥–åŠ± min', lambda history: f"{np.min(history["reward"]):.4f}")
    # env.add_metric(
    #     'å¥–åŠ± avg', lambda history: f"{np.mean(history["reward"]):.4f}")
    # env.add_metric(
    #     'å¥–åŠ± median', lambda history: f"{np.median(history["reward"]):.5f}")
    # env.add_metric('æœ€å¤§å›æ’¤', lambda history: f"{cal_max_drawdown(history):.2f}")
    # env.add_metric('IDX', lambda history: f"{history['idx',0]}")
    # env.add_metric('IDXLAST', lambda history: f"{history['idx',-1]}")

    eval_env = Monitor(env, './eval_logs')
    return env if not eval else eval_env


if __name__ == "__main__":
    # å‚æ•°è§£æ
    parser = argparse.ArgumentParser()
    parser.add_argument('--symbol', type=str, default='300604',
                        help='è‚¡ç¥¨ä»£ç ')

    args = parser.parse_args()
    symbol = args.symbol
    env = make_env(symbol, window_size=5, eval=False)
    for _ in range(2):
        terminated, truncated = False, False
        observation, info = env.reset()
        while not terminated or not truncated:
            action = env.action_space.sample()
            observation, reward, terminated, truncated, info = env.step(action)
            if terminated or truncated:
                obs, _ = env.reset()
            # print(observation,info)
            # print(env._features_columns)
    # Save for render
    # env.save_for_render()
